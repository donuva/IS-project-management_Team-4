{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd \"/content/drive/MyDrive/QLDA\"\n",
        "# !ls"
      ],
      "metadata": {
        "id": "I6_lESE-GljL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/OpenNMT/OpenNMT-py.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXatRVJ6aAxP",
        "outputId": "6b7ce4bb-bc32-49ed-f145-f5daf734b2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OpenNMT-py'...\n",
            "remote: Enumerating objects: 21869, done.\u001b[K\n",
            "remote: Counting objects: 100% (2717/2717), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1225/1225), done.\u001b[K\n",
            "remote: Total 21869 (delta 1692), reused 2312 (delta 1440), pack-reused 19152 (from 1)\u001b[K\n",
            "Receiving objects: 100% (21869/21869), 314.08 MiB | 18.18 MiB/s, done.\n",
            "Resolving deltas: 100% (15625/15625), done.\n",
            "Updating files: 100% (575/575), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/OpenNMT-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwuz9UJIaFXA",
        "outputId": "3e34d441-7b84-4349-87e5-b9df8eb5255a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/OpenNMT-py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout v1.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypL0hJ2jaF6-",
        "outputId": "1d152181-0bd1-4792-b820-e3ffbdb539d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'v1.2.0' set up to track remote branch 'v1.2.0' from 'origin'.\n",
            "Switched to a new branch 'v1.2.0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py install\n",
        "!pip install torchtext==0.4.0\n",
        "!pip install torch==1.4.0\n",
        "!pip install tqdm==4.30.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nFXDfNTair_",
        "outputId": "7af2449b-fd96-4b9a-936b-af618f560b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating OpenNMT_py.egg-info\n",
            "writing OpenNMT_py.egg-info/PKG-INFO\n",
            "writing dependency_links to OpenNMT_py.egg-info/dependency_links.txt\n",
            "writing entry points to OpenNMT_py.egg-info/entry_points.txt\n",
            "writing requirements to OpenNMT_py.egg-info/requires.txt\n",
            "writing top-level names to OpenNMT_py.egg-info/top_level.txt\n",
            "writing manifest file 'OpenNMT_py.egg-info/SOURCES.txt'\n",
            "reading manifest file 'OpenNMT_py.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE.md'\n",
            "writing manifest file 'OpenNMT_py.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/lib/onmt\n",
            "copying onmt/trainer.py -> build/lib/onmt\n",
            "copying onmt/model_builder.py -> build/lib/onmt\n",
            "copying onmt/opts.py -> build/lib/onmt\n",
            "copying onmt/train_single.py -> build/lib/onmt\n",
            "copying onmt/__init__.py -> build/lib/onmt\n",
            "creating build/lib/onmt/translate\n",
            "copying onmt/translate/translation_server.py -> build/lib/onmt/translate\n",
            "copying onmt/translate/penalties.py -> build/lib/onmt/translate\n",
            "copying onmt/translate/translation.py -> build/lib/onmt/translate\n",
            "copying onmt/translate/process_zh.py -> build/lib/onmt/translate\n",
            "copying onmt/translate/decode_strategy.py -> build/lib/onmt/translate\n",
            "copying onmt/translate/beam_search.py -> build/lib/onmt/translate\n",
            "copying onmt/translate/translator.py -> build/lib/onmt/translate\n",
            "copying onmt/translate/__init__.py -> build/lib/onmt/translate\n",
            "copying onmt/translate/greedy_search.py -> build/lib/onmt/translate\n",
            "creating build/lib/onmt/tests\n",
            "copying onmt/tests/test_embeddings.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_attention.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_image_dataset.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_copy_generator.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_preprocess.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_translation_server.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_structured_attention.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_greedy_search.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_audio_dataset.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_text_dataset.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_models.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_simple.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/test_beam_search.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/utils_for_tests.py -> build/lib/onmt/tests\n",
            "copying onmt/tests/__init__.py -> build/lib/onmt/tests\n",
            "creating build/lib/onmt/inputters\n",
            "copying onmt/inputters/vec_dataset.py -> build/lib/onmt/inputters\n",
            "copying onmt/inputters/inputter.py -> build/lib/onmt/inputters\n",
            "copying onmt/inputters/text_dataset.py -> build/lib/onmt/inputters\n",
            "copying onmt/inputters/image_dataset.py -> build/lib/onmt/inputters\n",
            "copying onmt/inputters/datareader_base.py -> build/lib/onmt/inputters\n",
            "copying onmt/inputters/audio_dataset.py -> build/lib/onmt/inputters\n",
            "copying onmt/inputters/dataset_base.py -> build/lib/onmt/inputters\n",
            "copying onmt/inputters/__init__.py -> build/lib/onmt/inputters\n",
            "creating build/lib/onmt/decoders\n",
            "copying onmt/decoders/decoder.py -> build/lib/onmt/decoders\n",
            "copying onmt/decoders/transformer.py -> build/lib/onmt/decoders\n",
            "copying onmt/decoders/cnn_decoder.py -> build/lib/onmt/decoders\n",
            "copying onmt/decoders/ensemble.py -> build/lib/onmt/decoders\n",
            "copying onmt/decoders/__init__.py -> build/lib/onmt/decoders\n",
            "creating build/lib/onmt/modules\n",
            "copying onmt/modules/copy_generator.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/global_attention.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/embeddings.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/source_noise.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/conv_multi_step_attention.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/position_ffn.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/average_attn.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/util_class.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/gate.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/structured_attention.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/sparse_losses.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/weight_norm.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/sparse_activations.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/multi_headed_attn.py -> build/lib/onmt/modules\n",
            "copying onmt/modules/__init__.py -> build/lib/onmt/modules\n",
            "creating build/lib/onmt/models\n",
            "copying onmt/models/model.py -> build/lib/onmt/models\n",
            "copying onmt/models/model_saver.py -> build/lib/onmt/models\n",
            "copying onmt/models/stacked_rnn.py -> build/lib/onmt/models\n",
            "copying onmt/models/sru.py -> build/lib/onmt/models\n",
            "copying onmt/models/__init__.py -> build/lib/onmt/models\n",
            "creating build/lib/onmt/utils\n",
            "copying onmt/utils/statistics.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/parse.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/misc.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/distributed.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/cnn_factory.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/logging.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/loss.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/report_manager.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/rnn_factory.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/optimizers.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/alignment.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/earlystopping.py -> build/lib/onmt/utils\n",
            "copying onmt/utils/__init__.py -> build/lib/onmt/utils\n",
            "creating build/lib/onmt/encoders\n",
            "copying onmt/encoders/cnn_encoder.py -> build/lib/onmt/encoders\n",
            "copying onmt/encoders/encoder.py -> build/lib/onmt/encoders\n",
            "copying onmt/encoders/audio_encoder.py -> build/lib/onmt/encoders\n",
            "copying onmt/encoders/rnn_encoder.py -> build/lib/onmt/encoders\n",
            "copying onmt/encoders/mean_encoder.py -> build/lib/onmt/encoders\n",
            "copying onmt/encoders/image_encoder.py -> build/lib/onmt/encoders\n",
            "copying onmt/encoders/ggnn_encoder.py -> build/lib/onmt/encoders\n",
            "copying onmt/encoders/transformer.py -> build/lib/onmt/encoders\n",
            "copying onmt/encoders/__init__.py -> build/lib/onmt/encoders\n",
            "creating build/lib/onmt/bin\n",
            "copying onmt/bin/server.py -> build/lib/onmt/bin\n",
            "copying onmt/bin/release_model.py -> build/lib/onmt/bin\n",
            "copying onmt/bin/translate.py -> build/lib/onmt/bin\n",
            "copying onmt/bin/average_models.py -> build/lib/onmt/bin\n",
            "copying onmt/bin/preprocess.py -> build/lib/onmt/bin\n",
            "copying onmt/bin/train.py -> build/lib/onmt/bin\n",
            "copying onmt/bin/__init__.py -> build/lib/onmt/bin\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/onmt\n",
            "creating build/bdist.linux-x86_64/egg/onmt/translate\n",
            "copying build/lib/onmt/translate/translation_server.py -> build/bdist.linux-x86_64/egg/onmt/translate\n",
            "copying build/lib/onmt/translate/penalties.py -> build/bdist.linux-x86_64/egg/onmt/translate\n",
            "copying build/lib/onmt/translate/translation.py -> build/bdist.linux-x86_64/egg/onmt/translate\n",
            "copying build/lib/onmt/translate/process_zh.py -> build/bdist.linux-x86_64/egg/onmt/translate\n",
            "copying build/lib/onmt/translate/decode_strategy.py -> build/bdist.linux-x86_64/egg/onmt/translate\n",
            "copying build/lib/onmt/translate/beam_search.py -> build/bdist.linux-x86_64/egg/onmt/translate\n",
            "copying build/lib/onmt/translate/translator.py -> build/bdist.linux-x86_64/egg/onmt/translate\n",
            "copying build/lib/onmt/translate/__init__.py -> build/bdist.linux-x86_64/egg/onmt/translate\n",
            "copying build/lib/onmt/translate/greedy_search.py -> build/bdist.linux-x86_64/egg/onmt/translate\n",
            "copying build/lib/onmt/trainer.py -> build/bdist.linux-x86_64/egg/onmt\n",
            "copying build/lib/onmt/model_builder.py -> build/bdist.linux-x86_64/egg/onmt\n",
            "creating build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_embeddings.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_attention.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_image_dataset.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_copy_generator.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_preprocess.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_translation_server.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_structured_attention.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_greedy_search.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_audio_dataset.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_text_dataset.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_models.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_simple.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/test_beam_search.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/utils_for_tests.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "copying build/lib/onmt/tests/__init__.py -> build/bdist.linux-x86_64/egg/onmt/tests\n",
            "creating build/bdist.linux-x86_64/egg/onmt/inputters\n",
            "copying build/lib/onmt/inputters/vec_dataset.py -> build/bdist.linux-x86_64/egg/onmt/inputters\n",
            "copying build/lib/onmt/inputters/inputter.py -> build/bdist.linux-x86_64/egg/onmt/inputters\n",
            "copying build/lib/onmt/inputters/text_dataset.py -> build/bdist.linux-x86_64/egg/onmt/inputters\n",
            "copying build/lib/onmt/inputters/image_dataset.py -> build/bdist.linux-x86_64/egg/onmt/inputters\n",
            "copying build/lib/onmt/inputters/datareader_base.py -> build/bdist.linux-x86_64/egg/onmt/inputters\n",
            "copying build/lib/onmt/inputters/audio_dataset.py -> build/bdist.linux-x86_64/egg/onmt/inputters\n",
            "copying build/lib/onmt/inputters/dataset_base.py -> build/bdist.linux-x86_64/egg/onmt/inputters\n",
            "copying build/lib/onmt/inputters/__init__.py -> build/bdist.linux-x86_64/egg/onmt/inputters\n",
            "creating build/bdist.linux-x86_64/egg/onmt/decoders\n",
            "copying build/lib/onmt/decoders/decoder.py -> build/bdist.linux-x86_64/egg/onmt/decoders\n",
            "copying build/lib/onmt/decoders/transformer.py -> build/bdist.linux-x86_64/egg/onmt/decoders\n",
            "copying build/lib/onmt/decoders/cnn_decoder.py -> build/bdist.linux-x86_64/egg/onmt/decoders\n",
            "copying build/lib/onmt/decoders/ensemble.py -> build/bdist.linux-x86_64/egg/onmt/decoders\n",
            "copying build/lib/onmt/decoders/__init__.py -> build/bdist.linux-x86_64/egg/onmt/decoders\n",
            "creating build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/copy_generator.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/global_attention.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/embeddings.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/source_noise.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/conv_multi_step_attention.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/position_ffn.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/average_attn.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/util_class.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/gate.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/structured_attention.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/sparse_losses.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/weight_norm.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/sparse_activations.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/multi_headed_attn.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "copying build/lib/onmt/modules/__init__.py -> build/bdist.linux-x86_64/egg/onmt/modules\n",
            "creating build/bdist.linux-x86_64/egg/onmt/models\n",
            "copying build/lib/onmt/models/model.py -> build/bdist.linux-x86_64/egg/onmt/models\n",
            "copying build/lib/onmt/models/model_saver.py -> build/bdist.linux-x86_64/egg/onmt/models\n",
            "copying build/lib/onmt/models/stacked_rnn.py -> build/bdist.linux-x86_64/egg/onmt/models\n",
            "copying build/lib/onmt/models/sru.py -> build/bdist.linux-x86_64/egg/onmt/models\n",
            "copying build/lib/onmt/models/__init__.py -> build/bdist.linux-x86_64/egg/onmt/models\n",
            "creating build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/statistics.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/parse.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/misc.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/distributed.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/cnn_factory.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/logging.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/loss.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/report_manager.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/rnn_factory.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/optimizers.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/alignment.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/earlystopping.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/utils/__init__.py -> build/bdist.linux-x86_64/egg/onmt/utils\n",
            "copying build/lib/onmt/opts.py -> build/bdist.linux-x86_64/egg/onmt\n",
            "creating build/bdist.linux-x86_64/egg/onmt/encoders\n",
            "copying build/lib/onmt/encoders/cnn_encoder.py -> build/bdist.linux-x86_64/egg/onmt/encoders\n",
            "copying build/lib/onmt/encoders/encoder.py -> build/bdist.linux-x86_64/egg/onmt/encoders\n",
            "copying build/lib/onmt/encoders/audio_encoder.py -> build/bdist.linux-x86_64/egg/onmt/encoders\n",
            "copying build/lib/onmt/encoders/rnn_encoder.py -> build/bdist.linux-x86_64/egg/onmt/encoders\n",
            "copying build/lib/onmt/encoders/mean_encoder.py -> build/bdist.linux-x86_64/egg/onmt/encoders\n",
            "copying build/lib/onmt/encoders/image_encoder.py -> build/bdist.linux-x86_64/egg/onmt/encoders\n",
            "copying build/lib/onmt/encoders/ggnn_encoder.py -> build/bdist.linux-x86_64/egg/onmt/encoders\n",
            "copying build/lib/onmt/encoders/transformer.py -> build/bdist.linux-x86_64/egg/onmt/encoders\n",
            "copying build/lib/onmt/encoders/__init__.py -> build/bdist.linux-x86_64/egg/onmt/encoders\n",
            "creating build/bdist.linux-x86_64/egg/onmt/bin\n",
            "copying build/lib/onmt/bin/server.py -> build/bdist.linux-x86_64/egg/onmt/bin\n",
            "copying build/lib/onmt/bin/release_model.py -> build/bdist.linux-x86_64/egg/onmt/bin\n",
            "copying build/lib/onmt/bin/translate.py -> build/bdist.linux-x86_64/egg/onmt/bin\n",
            "copying build/lib/onmt/bin/average_models.py -> build/bdist.linux-x86_64/egg/onmt/bin\n",
            "copying build/lib/onmt/bin/preprocess.py -> build/bdist.linux-x86_64/egg/onmt/bin\n",
            "copying build/lib/onmt/bin/train.py -> build/bdist.linux-x86_64/egg/onmt/bin\n",
            "copying build/lib/onmt/bin/__init__.py -> build/bdist.linux-x86_64/egg/onmt/bin\n",
            "copying build/lib/onmt/train_single.py -> build/bdist.linux-x86_64/egg/onmt\n",
            "copying build/lib/onmt/__init__.py -> build/bdist.linux-x86_64/egg/onmt\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/translate/translation_server.py to translation_server.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/translate/penalties.py to penalties.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/translate/translation.py to translation.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/translate/process_zh.py to process_zh.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/translate/decode_strategy.py to decode_strategy.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/translate/beam_search.py to beam_search.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/translate/translator.py to translator.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/translate/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/translate/greedy_search.py to greedy_search.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/trainer.py to trainer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/model_builder.py to model_builder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_embeddings.py to test_embeddings.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_attention.py to test_attention.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_image_dataset.py to test_image_dataset.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_copy_generator.py to test_copy_generator.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_preprocess.py to test_preprocess.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_translation_server.py to test_translation_server.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_structured_attention.py to test_structured_attention.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_greedy_search.py to test_greedy_search.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_audio_dataset.py to test_audio_dataset.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_text_dataset.py to test_text_dataset.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_models.py to test_models.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_simple.py to test_simple.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/test_beam_search.py to test_beam_search.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/utils_for_tests.py to utils_for_tests.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/tests/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/inputters/vec_dataset.py to vec_dataset.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/inputters/inputter.py to inputter.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/inputters/text_dataset.py to text_dataset.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/inputters/image_dataset.py to image_dataset.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/inputters/datareader_base.py to datareader_base.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/inputters/audio_dataset.py to audio_dataset.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/inputters/dataset_base.py to dataset_base.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/inputters/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/decoders/decoder.py to decoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/decoders/transformer.py to transformer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/decoders/cnn_decoder.py to cnn_decoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/decoders/ensemble.py to ensemble.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/decoders/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/copy_generator.py to copy_generator.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/global_attention.py to global_attention.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/embeddings.py to embeddings.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/source_noise.py to source_noise.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/conv_multi_step_attention.py to conv_multi_step_attention.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/position_ffn.py to position_ffn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/average_attn.py to average_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/util_class.py to util_class.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/gate.py to gate.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/structured_attention.py to structured_attention.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/sparse_losses.py to sparse_losses.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/weight_norm.py to weight_norm.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/sparse_activations.py to sparse_activations.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/multi_headed_attn.py to multi_headed_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/modules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/models/model.py to model.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/models/model_saver.py to model_saver.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/models/stacked_rnn.py to stacked_rnn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/models/sru.py to sru.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/models/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/statistics.py to statistics.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/parse.py to parse.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/misc.py to misc.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/distributed.py to distributed.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/cnn_factory.py to cnn_factory.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/logging.py to logging.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/loss.py to loss.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/report_manager.py to report_manager.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/rnn_factory.py to rnn_factory.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/optimizers.py to optimizers.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/alignment.py to alignment.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/earlystopping.py to earlystopping.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/utils/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/opts.py to opts.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/encoders/cnn_encoder.py to cnn_encoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/encoders/encoder.py to encoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/encoders/audio_encoder.py to audio_encoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/encoders/rnn_encoder.py to rnn_encoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/encoders/mean_encoder.py to mean_encoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/encoders/image_encoder.py to image_encoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/encoders/ggnn_encoder.py to ggnn_encoder.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/encoders/transformer.py to transformer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/encoders/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/bin/server.py to server.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/bin/release_model.py to release_model.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/bin/translate.py to translate.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/bin/average_models.py to average_models.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/bin/preprocess.py to preprocess.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/bin/train.py to train.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/bin/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/train_single.py to train_single.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/onmt/__init__.py to __init__.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying OpenNMT_py.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "onmt.tests.__pycache__.test_audio_dataset.cpython-310: module references __file__\n",
            "onmt.tests.__pycache__.test_image_dataset.cpython-310: module references __file__\n",
            "onmt.tests.__pycache__.test_translation_server.cpython-310: module references __file__\n",
            "creating dist\n",
            "creating 'dist/OpenNMT_py-1.2.0-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing OpenNMT_py-1.2.0-py3.10.egg\n",
            "creating /usr/local/lib/python3.10/dist-packages/OpenNMT_py-1.2.0-py3.10.egg\n",
            "Extracting OpenNMT_py-1.2.0-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding OpenNMT-py 1.2.0 to easy-install.pth file\n",
            "Installing onmt_average_models script to /usr/local/bin\n",
            "Installing onmt_preprocess script to /usr/local/bin\n",
            "Installing onmt_release_model script to /usr/local/bin\n",
            "Installing onmt_server script to /usr/local/bin\n",
            "Installing onmt_train script to /usr/local/bin\n",
            "Installing onmt_translate script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/OpenNMT_py-1.2.0-py3.10.egg\n",
            "Processing dependencies for OpenNMT-py==1.2.0\n",
            "Searching for pyonmttok==1.*\n",
            "Reading https://pypi.org/simple/pyonmttok/\n",
            "Downloading https://files.pythonhosted.org/packages/09/91/bc1a1e7d38913b0eca17d2dcef3606ef2f65d57a4971ad64af8607c78aaf/pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl#sha256=0cdfc6cebbf4921fcfcb4e76388142c777c025d014bd79d9a7858bba54b04129\n",
            "Best match: pyonmttok 1.37.1\n",
            "Processing pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
            "Installing pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding pyonmttok 1.37.1 to easy-install.pth file\n",
            "detected new path './OpenNMT_py-1.2.0-py3.10.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/pyonmttok-1.37.1-py3.10-linux-x86_64.egg\n",
            "Searching for waitress\n",
            "Reading https://pypi.org/simple/waitress/\n",
            "Downloading https://files.pythonhosted.org/packages/5b/a9/485c953a1ac4cb98c28e41fd2c7184072df36bbf99734a51d44d04176878/waitress-3.0.0-py3-none-any.whl#sha256=2a06f242f4ba0cc563444ca3d1998959447477363a2d7e9b8b4d75d35cfd1669\n",
            "Best match: waitress 3.0.0\n",
            "Processing waitress-3.0.0-py3-none-any.whl\n",
            "Installing waitress-3.0.0-py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding waitress 3.0.0 to easy-install.pth file\n",
            "detected new path './pyonmttok-1.37.1-py3.10-linux-x86_64.egg'\n",
            "Installing waitress-serve script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/waitress-3.0.0-py3.10.egg\n",
            "Searching for configargparse\n",
            "Reading https://pypi.org/simple/configargparse/\n",
            "Downloading https://files.pythonhosted.org/packages/6f/b3/b4ac838711fd74a2b4e6f746703cf9dd2cf5462d17dac07e349234e21b97/ConfigArgParse-1.7-py3-none-any.whl#sha256=d249da6591465c6c26df64a9f73d2536e743be2f244eb3ebe61114af2f94f86b\n",
            "Best match: ConfigArgParse 1.7\n",
            "Processing ConfigArgParse-1.7-py3-none-any.whl\n",
            "Installing ConfigArgParse-1.7-py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding ConfigArgParse 1.7 to easy-install.pth file\n",
            "detected new path './waitress-3.0.0-py3.10.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/ConfigArgParse-1.7-py3.10.egg\n",
            "Searching for torchtext==0.4.0\n",
            "Reading https://pypi.org/simple/torchtext/\n",
            "Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl#sha256=094520d9cd0af6a05368d9023fdc91dc038232bd9d128c7b548ec2200dba53ec\n",
            "Best match: torchtext 0.4.0\n",
            "Processing torchtext-0.4.0-py3-none-any.whl\n",
            "Installing torchtext-0.4.0-py3-none-any.whl to /usr/local/lib/python3.10/dist-packages\n",
            "Adding torchtext 0.4.0 to easy-install.pth file\n",
            "detected new path './ConfigArgParse-1.7-py3.10.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/torchtext-0.4.0-py3.10.egg\n",
            "Searching for PyYAML==6.0.2\n",
            "Best match: PyYAML 6.0.2\n",
            "Adding PyYAML 6.0.2 to easy-install.pth file\n",
            "detected new path './torchtext-0.4.0-py3.10.egg'\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for Flask==2.2.5\n",
            "Best match: Flask 2.2.5\n",
            "Adding Flask 2.2.5 to easy-install.pth file\n",
            "Installing flask script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tensorboard==2.17.0\n",
            "Best match: tensorboard 2.17.0\n",
            "Adding tensorboard 2.17.0 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for future==1.0.0\n",
            "Best match: future 1.0.0\n",
            "Adding future 1.0.0 to easy-install.pth file\n",
            "Installing futurize script to /usr/local/bin\n",
            "Installing pasteurize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for torch==2.5.0+cu121\n",
            "Best match: torch 2.5.0+cu121\n",
            "Adding torch 2.5.0+cu121 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "Installing torchfrtrace script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tqdm==4.66.5\n",
            "Best match: tqdm 4.66.5\n",
            "Adding tqdm 4.66.5 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for six==1.16.0\n",
            "Best match: six 1.16.0\n",
            "Adding six 1.16.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for click==8.1.7\n",
            "Best match: click 8.1.7\n",
            "Adding click 8.1.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for itsdangerous==2.2.0\n",
            "Best match: itsdangerous 2.2.0\n",
            "Adding itsdangerous 2.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for jinja2==3.1.4\n",
            "Best match: jinja2 3.1.4\n",
            "Adding jinja2 3.1.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for werkzeug==3.0.4\n",
            "Best match: werkzeug 3.0.4\n",
            "Adding werkzeug 3.0.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for tensorboard-data-server==0.7.2\n",
            "Best match: tensorboard-data-server 0.7.2\n",
            "Adding tensorboard-data-server 0.7.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for setuptools==75.1.0\n",
            "Best match: setuptools 75.1.0\n",
            "Adding setuptools 75.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for protobuf==3.20.3\n",
            "Best match: protobuf 3.20.3\n",
            "Adding protobuf 3.20.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for numpy==1.26.4\n",
            "Best match: numpy 1.26.4\n",
            "Adding numpy 1.26.4 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for Markdown==3.7\n",
            "Best match: Markdown 3.7\n",
            "Adding Markdown 3.7 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for grpcio==1.64.1\n",
            "Best match: grpcio 1.64.1\n",
            "Adding grpcio 1.64.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for absl-py==1.4.0\n",
            "Best match: absl-py 1.4.0\n",
            "Adding absl-py 1.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for requests==2.32.3\n",
            "Best match: requests 2.32.3\n",
            "Adding requests 2.32.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for sympy==1.13.1\n",
            "Best match: sympy 1.13.1\n",
            "Adding sympy 1.13.1 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for fsspec==2024.6.1\n",
            "Best match: fsspec 2024.6.1\n",
            "Adding fsspec 2024.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for networkx==3.4.2\n",
            "Best match: networkx 3.4.2\n",
            "Adding networkx 3.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for typing-extensions==4.12.2\n",
            "Best match: typing-extensions 4.12.2\n",
            "Adding typing-extensions 4.12.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages/setuptools/_vendor\n",
            "Searching for filelock==3.16.1\n",
            "Best match: filelock 3.16.1\n",
            "Adding filelock 3.16.1 to easy-install.pth file\n",
            "detected new path './setuptools/_vendor'\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for MarkupSafe==3.0.2\n",
            "Best match: MarkupSafe 3.0.2\n",
            "Adding MarkupSafe 3.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for certifi==2024.8.30\n",
            "Best match: certifi 2024.8.30\n",
            "Adding certifi 2024.8.30 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for urllib3==2.2.3\n",
            "Best match: urllib3 2.2.3\n",
            "Adding urllib3 2.2.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for idna==3.10\n",
            "Best match: idna 3.10\n",
            "Adding idna 3.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for charset-normalizer==3.4.0\n",
            "Best match: charset-normalizer 3.4.0\n",
            "Adding charset-normalizer 3.4.0 to easy-install.pth file\n",
            "Installing normalizer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Searching for mpmath==1.3.0\n",
            "Best match: mpmath 1.3.0\n",
            "Adding mpmath 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.10/dist-packages\n",
            "Finished processing dependencies for OpenNMT-py==1.2.0\n",
            "Requirement already satisfied: torchtext==0.4.0 in /usr/local/lib/python3.10/dist-packages/torchtext-0.4.0-py3.10.egg (0.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4.0) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4.0) (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4.0) (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4.0) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4.0) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torchtext==0.4.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.4.0) (3.0.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.4.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.4.0\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting tqdm==4.30.0\n",
            "  Downloading tqdm-4.30.0-py2.py3-none-any.whl.metadata (37 kB)\n",
            "Downloading tqdm-4.30.0-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires tqdm>=4.64.1, but you have tqdm 4.30.0 which is incompatible.\n",
            "huggingface-hub 0.24.7 requires tqdm>=4.42.1, but you have tqdm 4.30.0 which is incompatible.\n",
            "panel 1.4.5 requires tqdm>=4.48.0, but you have tqdm 4.30.0 which is incompatible.\n",
            "prophet 1.1.6 requires tqdm>=4.36.1, but you have tqdm 4.30.0 which is incompatible.\n",
            "spacy 3.7.5 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.30.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tqdm-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/QLDA\"\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN9VUfdKa31r",
        "outputId": "b92c1dab-d7eb-418f-d239-ce9dc0a45565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/QLDA\n",
            "article   img\t\t test_doc_name.txt  tgt-test.txt\ttrain_document.txt  truncated_docs\n",
            "caption   src_test.txt\t test_document.txt  tgt-train.txt\tTrain_Image_names   url\n",
            "data.txt  src_train.txt  test_title.txt     train_doc_name.txt\ttrain_title.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXTRACTION"
      ],
      "metadata": {
        "id": "R8Y6Am0aN7hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "base = \"/content/drive/MyDrive/QLDA\""
      ],
      "metadata": {
        "id": "T8YXhiEVGyOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_list = os.listdir(base + '/article')\n",
        "print(len(article_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knl_x11tHwJ7",
        "outputId": "e8ec6863-1d73-448a-b4d4-d2e2dac74949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_list[0:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2KPZsd5L7Vu",
        "outputId": "0aa4dce3-6137-4d6d-b9d9-429825a68cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['880f411f0c4fc8528c299c0223173d457d2b0049.txt',\n",
              " '1cedb09c9e20c08bdef71f98fe43e4677a9709db.txt',\n",
              " '48254ccb210b9f3c833877790c72a2afe1899242.txt',\n",
              " '5fb424d964ba019649f640cdf06945aaf539c4c6.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN_DATA\n",
        "#\n",
        "head=open('train_title.txt','w')\n",
        "document=open('train_document.txt','w')\n",
        "doc_name=open('train_doc_name.txt','w')\n",
        "\n",
        "for article_name in article_list[0:4]:\n",
        "  with open(base +'/article/'+ article_name,'r') as article:\n",
        "      print(article_name)\n",
        "      art=article.read()\n",
        "      x=re.sub(r'\\s',' ',art)\n",
        "      x=re.sub('\\@body','\\n@body',x)\n",
        "      x=re.sub('\\@summary','\\n@summary',x)\n",
        "      with open(\"data.txt\",'w') as f:\n",
        "          f.write(x)\n",
        "      with open(\"data.txt\",'r') as f:\n",
        "          doc=f.readlines()\n",
        "      summary=[]\n",
        "      for i in doc:\n",
        "          i=re.sub(r'\\s',' ',i)\n",
        "          if '@title' in i:\n",
        "              title=i\n",
        "          if '@body' in i:\n",
        "              body=i\n",
        "\n",
        "      if len(body.split())> 25 and len(title.split())> 3 :\n",
        "          head.write(re.sub('\\@title','',title)+\"\\n\")\n",
        "          document.write(re.sub('\\@body','',body)+\"\\n\")\n",
        "          doc_name.write(article_name+'\\n')\n",
        "\n",
        "#\n",
        "head.close()\n",
        "document.close()\n",
        "doc_name.close()\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3Htem7kG2Bu",
        "outputId": "8b9de1a3-8e38-43d6-b4a7-68aa87843dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "880f411f0c4fc8528c299c0223173d457d2b0049.txt\n",
            "1cedb09c9e20c08bdef71f98fe43e4677a9709db.txt\n",
            "48254ccb210b9f3c833877790c72a2afe1899242.txt\n",
            "5fb424d964ba019649f640cdf06945aaf539c4c6.txt\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST_DATA\n",
        "#\n",
        "head=open('test_title.txt','w')\n",
        "document=open('test_document.txt','w')\n",
        "doc_name=open('test_doc_name.txt','w')\n",
        "\n",
        "for article_name in article_list[5:9]:\n",
        "  with open(base +'/article/'+ article_name,'r') as article:\n",
        "      print(article_name)\n",
        "      art=article.read()\n",
        "      x=re.sub(r'\\s',' ',art)\n",
        "      x=re.sub('\\@body','\\n@body',x)\n",
        "      x=re.sub('\\@summary','\\n@summary',x)\n",
        "      with open(\"data.txt\",'w') as f:\n",
        "          f.write(x)\n",
        "      with open(\"data.txt\",'r') as f:\n",
        "          doc=f.readlines()\n",
        "      summary=[]\n",
        "      for i in doc:\n",
        "          i=re.sub(r'\\s',' ',i)\n",
        "          if '@title' in i:\n",
        "              title=i\n",
        "          if '@body' in i:\n",
        "              body=i\n",
        "\n",
        "      if len(body.split())> 25 and len(title.split())> 3 :\n",
        "          head.write(re.sub('\\@title','',title)+\"\\n\")\n",
        "          document.write(re.sub('\\@body','',body)+\"\\n\")\n",
        "          doc_name.write(article_name+'\\n')\n",
        "\n",
        "#\n",
        "head.close()\n",
        "document.close()\n",
        "doc_name.close()\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ouCCbYuHRKH",
        "outputId": "7b8371e3-e0e3-4c60-a7cc-50dbdd9e2dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0f08fcb4e2129c363ffd357aa317d31e39ed75cd.txt\n",
            "40fec133fa71b6a6f477f13661afbf3c7b8b6b9e.txt\n",
            "18637d56630bb817b4760f06bc40b43d21e712fd.txt\n",
            "b48a17e26f873de580bdddc5ba628ef537ead15b.txt\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLEAN"
      ],
      "metadata": {
        "id": "sVoQWJR0OCFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_mapping = {\n",
        "    \"ain't\": \"is not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"they've\": \"they have\"\n",
        "}"
      ],
      "metadata": {
        "id": "hucZkA8ePEQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning(doc,contraction_mapping):\n",
        "    clean=[]\n",
        "    for i in tqdm(doc):\n",
        "        low=str(i).lower()\n",
        "        soup=BeautifulSoup(low,'lxml')\n",
        "        low=soup.text\n",
        "        low=re.sub(\" '\",\"'\",low)\n",
        "        low=re.sub(\" n't\",\"n't\",low)\n",
        "        sent=[]\n",
        "        for m in (low.split()):\n",
        "            if m in contraction_mapping:\n",
        "                sent.append(contraction_mapping[m])\n",
        "            else:\n",
        "                sent.append(m)\n",
        "        jnt=' '.join(sent)\n",
        "        low=re.sub(\"'s\",\"\",jnt)\n",
        "        low=re.sub(\"’s\",\"\",low)\n",
        "        new=re.sub(\"\\n\",'',low)\n",
        "        new=re.sub(r'[\\$\\\"\\(\\)\\)\\#\\:\\@\\=\\>\\<\\-\\`\\-\\-\\/\\;\\‘\\£\\%\\*\\—]',' ',new)\n",
        "        new=re.sub(\",\",'',new)\n",
        "        new=re.sub('\\!','.',new)\n",
        "        new=re.sub('\\?','.',new)\n",
        "        new=re.sub(\"'\",'',new)\n",
        "        new=re.sub(\"°\",'',new)\n",
        "        new=re.sub(\"\\.\\.\\.\",'.',new)\n",
        "        new=re.sub(r\"[^a-zA-Z0-9]\",' ',new)\n",
        "        new=(re.sub(r'[\\s]+',' ',new)).strip()\n",
        "        clean.append(new)\n",
        "    return clean"
      ],
      "metadata": {
        "id": "zTk7WPlnOZv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "print(\"Cleaning Source Training Data\")\n",
        "with open(\"train_document.txt\",'r') as file:\n",
        "    doc=file.readlines()\n",
        "final_data=cleaning(doc,contraction_mapping)\n",
        "\n",
        "with open(\"src_train.txt\",'w') as file:\n",
        "    for summary in tqdm(final_data):\n",
        "        file.write(summary+'\\n')\n",
        "\n",
        "#\n",
        "print(\"Cleaning Target Training Data\")\n",
        "with open(\"train_title.txt\",'r') as file:\n",
        "    doc=file.readlines()\n",
        "final_data=cleaning(doc,contraction_mapping)\n",
        "\n",
        "with open(\"tgt-train.txt\",'w') as file:\n",
        "    for summary in tqdm(final_data):\n",
        "        file.write(summary+'\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "822ZEYFlN3gw",
        "outputId": "f390c522-0d3a-43f5-d3cf-a925c983eeb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 595.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning Source Training Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 3620.46it/s]\n",
            "100%|██████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 1564.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning Target Training Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 5061.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "print(\"Cleaning Source Test Data\")\n",
        "with open(\"test_document.txt\",'r') as file:\n",
        "    doc=file.readlines()\n",
        "final_data=cleaning(doc,contraction_mapping)\n",
        "\n",
        "with open(\"src_test.txt\",'w') as file:\n",
        "    for summary in tqdm(final_data):\n",
        "        file.write(summary+'\\n')\n",
        "\n",
        "#\n",
        "print(\"Cleaning Target Test Data\")\n",
        "with open(\"test_title.txt\",'r') as file:\n",
        "    doc=file.readlines()\n",
        "final_data=cleaning(doc,contraction_mapping)\n",
        "\n",
        "with open(\"tgt-test.txt\",'w') as file:\n",
        "    for summary in tqdm(final_data):\n",
        "        file.write(summary+'\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUrsGdfcQc8t",
        "outputId": "651afae5-9aa0-4f72-a271-968afca2b815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 873.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning Source Test Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 4815.50it/s]\n",
            "100%|██████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 1788.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning Target Test Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 3637.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMAGE CAPTION"
      ],
      "metadata": {
        "id": "RK-R6p2cQ4Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Re-Normalize doc_name\n",
        "import re\n",
        "\n",
        "with open(\"test_doc_name.txt\",'r') as doc:\n",
        "  article_list = doc.readlines()\n",
        "\n",
        "clean_ar_list=[]\n",
        "for i in article_list:\n",
        "  d1 = re.sub('\\n',\"\", i)\n",
        "  d2 = re.sub('\\.txt',\"\", d1)\n",
        "  clean_ar_list.append(d2)\n",
        "\n",
        "clean_ar_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8535sDElQy-Q",
        "outputId": "8cafa908-a16c-4b2c-cf69-e6b2af0315e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0f08fcb4e2129c363ffd357aa317d31e39ed75cd',\n",
              " '40fec133fa71b6a6f477f13661afbf3c7b8b6b9e',\n",
              " '18637d56630bb817b4760f06bc40b43d21e712fd',\n",
              " 'b48a17e26f873de580bdddc5ba628ef537ead15b']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "image_fold = os.listdir(\"/content/drive/MyDrive/QLDA/img\")"
      ],
      "metadata": {
        "id": "SoqpDi6jToJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_list = []\n",
        "clean_ar_list = ['00b27060a4e833ac1867db07fd2fa1aafb63bf46']\n",
        "for article_i in clean_ar_list:\n",
        "  article_i_image = []\n",
        "  for image_i in image_fold:\n",
        "    #print(article_i, image_i)\n",
        "    if article_i in image_i:\n",
        "      article_i_image.append(re.sub('\\.jpg','',image_i))\n",
        "  #\n",
        "  if article_i_image == []:\n",
        "    image_list.append('None')\n",
        "  else:\n",
        "    image_list.append(article_i_image)"
      ],
      "metadata": {
        "id": "LXUpYDjJS-Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIVyYdFBVH_-",
        "outputId": "cf413f15-7337-464f-942d-05617524ba45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['00b27060a4e833ac1867db07fd2fa1aafb63bf46_2',\n",
              "  '00b27060a4e833ac1867db07fd2fa1aafb63bf46_3',\n",
              "  '00b27060a4e833ac1867db07fd2fa1aafb63bf46_1',\n",
              "  '00b27060a4e833ac1867db07fd2fa1aafb63bf46_4']]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save\n",
        "with open(\"Train_Image_names\", \"w\") as file:\n",
        "  for i in image_list:\n",
        "    file.write('\\t'.join(i)+'\\n')\n",
        "\n",
        "file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x5u4y0tXy6U",
        "outputId": "401c8a31-3d17-42b1-baa2-7a02109a398a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.TextIOWrapper name='Train_Image_names' mode='w' encoding='UTF-8'>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAIN"
      ],
      "metadata": {
        "id": "UCn6JPq-Yk06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/OpenNMT-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUw7Gf6lbqzC",
        "outputId": "3fdbd86b-e1bc-4b7e-cae2-cd915ebc3eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/OpenNMT-py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocess.py -train_src \"/content/drive/MyDrive/QLDA/src_train.txt\" -train_tgt \"/content/drive/MyDrive/QLDA/tgt-train.txt\"  -save_data data/data --src_seq_length 110 --src_seq_length_trunc 110 --tgt_seq_length 26 --tgt_seq_length_trunc 26 --src_vocab_size 2300 --tgt_vocab_size 8500 -overwrite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2_xD0NNbrx9",
        "outputId": "ce0014a2-361c-4d9d-8928-e133e371e863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/OpenNMT-py/onmt/modules/sparse_activations.py:48: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, dim=0):\n",
            "/content/OpenNMT-py/onmt/modules/sparse_activations.py:68: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/content/OpenNMT-py/onmt/modules/sparse_losses.py:13: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, target):\n",
            "/content/OpenNMT-py/onmt/modules/sparse_losses.py:37: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/content/OpenNMT-py/onmt/models/sru.py:397: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(self, u, x, bias, init=None, mask_h=None):\n",
            "/content/OpenNMT-py/onmt/models/sru.py:443: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(self, grad_h, grad_last):\n",
            "[2024-10-28 09:37:26,532 INFO] Extracting features...\n",
            "[2024-10-28 09:37:26,538 INFO]  * number of source features: 0.\n",
            "[2024-10-28 09:37:26,538 INFO]  * number of target features: 0.\n",
            "[2024-10-28 09:37:26,538 INFO] Building `Fields` object...\n",
            "[2024-10-28 09:37:26,538 INFO] Building & saving training data...\n",
            "[2024-10-28 09:37:26,556 INFO] Building shard 0.\n",
            "[2024-10-28 09:37:26,558 INFO]  * saving 0th train data shard to data/data.train.0.pt.\n",
            "[2024-10-28 09:37:26,780 INFO]  * tgt vocab size: 86.\n",
            "[2024-10-28 09:37:26,780 INFO]  * src vocab size: 273.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py -data data/data -save_model \"/content/sample_data\" \\\n",
        "           --valid_steps 10 --valid_batch_size 128 --save_checkpoint_steps 10 \\\n",
        "           -global_attention mlp \\\n",
        "           -word_vec_size 128 \\\n",
        "           -rnn_size 512 \\\n",
        "           -layers 1 \\\n",
        "           -encoder_type brnn \\\n",
        "           -train_steps 70 \\\n",
        "           -max_grad_norm 2 \\\n",
        "           -dropout 0.4 \\\n",
        "           -batch_size 128 \\\n",
        "           -optim adagrad \\\n",
        "           -learning_rate 0.15 \\\n",
        "           -adagrad_accumulator_init 0.1 \\\n",
        "           -gpu_ranks 0\n",
        "           #-train_from '/content/OpenNMT-py/data/drive/My Drive/demo-model_step_37000.pt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvFWGX6Fdk4W",
        "outputId": "fe3f5daf-fc14-4034-8190-364486077c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/OpenNMT-py/onmt/modules/sparse_activations.py:48: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, dim=0):\n",
            "/content/OpenNMT-py/onmt/modules/sparse_activations.py:68: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/content/OpenNMT-py/onmt/modules/sparse_losses.py:13: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, target):\n",
            "/content/OpenNMT-py/onmt/modules/sparse_losses.py:37: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/content/OpenNMT-py/onmt/models/sru.py:397: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(self, u, x, bias, init=None, mask_h=None):\n",
            "/content/OpenNMT-py/onmt/models/sru.py:443: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(self, grad_h, grad_last):\n",
            "/content/OpenNMT-py/onmt/bin/train.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vocab = torch.load(opt.data + '.vocab.pt')\n",
            "/content/OpenNMT-py/onmt/inputters/inputter.py:193: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dvocab = torch.load(opt.data + '.vocab.pt')\n",
            "/content/OpenNMT-py/onmt/train_single.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vocab = torch.load(opt.data + '.vocab.pt')\n",
            "[2024-10-28 09:37:31,444 INFO]  * src vocab size = 273\n",
            "[2024-10-28 09:37:31,444 INFO]  * tgt vocab size = 86\n",
            "[2024-10-28 09:37:31,444 INFO] Building model...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
            "  warnings.warn(\n",
            "[2024-10-28 09:37:31,775 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(273, 128, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(128, 256, dropout=0.4, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(86, 128, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.4, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.4, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(640, 512)\n",
            "      )\n",
            "    )\n",
            "    (attn): GlobalAttention(\n",
            "      (linear_context): Linear(in_features=512, out_features=512, bias=False)\n",
            "      (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
            "      (linear_out): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=86, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2024-10-28 09:37:31,775 INFO] encoder: 825472\n",
            "[2024-10-28 09:37:31,775 INFO] decoder: 3468630\n",
            "[2024-10-28 09:37:31,775 INFO] * number of parameters: 4294102\n",
            "[2024-10-28 09:37:31,784 INFO] Starting training on GPU: [0]\n",
            "[2024-10-28 09:37:31,784 INFO] Start training loop without validation...\n",
            "[2024-10-28 09:37:31,784 INFO] Loading dataset from data/data.train.0.pt\n",
            "/content/OpenNMT-py/onmt/inputters/inputter.py:838: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  cur_dataset = torch.load(path)\n",
            "[2024-10-28 09:37:31,784 INFO] number of examples: 4\n",
            "/content/OpenNMT-py/onmt/trainer.py:388: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=self.optim.amp):\n",
            "[2024-10-28 09:37:34,084 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:34,085 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:34,163 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:34,164 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:34,236 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:34,237 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:34,319 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:34,320 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:34,411 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:34,412 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:34,495 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:34,496 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:34,587 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:34,589 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:34,703 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:34,704 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:34,802 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:34,803 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:34,902 INFO] Saving checkpoint /content/sample_data_step_10.pt\n",
            "[2024-10-28 09:37:34,983 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:34,984 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,074 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,075 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,166 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,167 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,252 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,254 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,339 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,340 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,424 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,425 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,516 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,517 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,614 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,615 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,715 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,716 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,773 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,774 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,832 INFO] Saving checkpoint /content/sample_data_step_20.pt\n",
            "[2024-10-28 09:37:35,889 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,890 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:35,952 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:35,953 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,015 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,015 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,072 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,073 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,133 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,134 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,194 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,195 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,254 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,255 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,313 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,314 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,372 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,373 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,434 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,434 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,495 INFO] Saving checkpoint /content/sample_data_step_30.pt\n",
            "[2024-10-28 09:37:36,562 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,562 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,665 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,667 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,746 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,747 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,813 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,813 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,891 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,892 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:36,957 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:36,958 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,017 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,018 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,083 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,084 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,146 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,146 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,204 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,205 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,269 INFO] Saving checkpoint /content/sample_data_step_40.pt\n",
            "[2024-10-28 09:37:37,317 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,318 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,378 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,379 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,436 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,437 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,497 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,498 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,556 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,557 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,620 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,621 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,682 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,682 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,752 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,753 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,809 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,810 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,868 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,868 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:37,928 INFO] Step 50/   70; acc:   6.90; ppl: 44.61; xent: 3.80; lr: 0.15000; 3581/757 tok/s;      6 sec\n",
            "[2024-10-28 09:37:37,929 INFO] Saving checkpoint /content/sample_data_step_50.pt\n",
            "[2024-10-28 09:37:37,978 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:37,979 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,040 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,041 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,097 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,097 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,156 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,156 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,218 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,218 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,274 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,275 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,331 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,331 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,389 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,389 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,449 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,449 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,509 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,510 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,570 INFO] Saving checkpoint /content/sample_data_step_60.pt\n",
            "[2024-10-28 09:37:38,621 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,622 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,682 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,683 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,749 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,750 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,813 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,814 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,880 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,881 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:38,943 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:38,944 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:39,001 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:39,002 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:39,058 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:39,059 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:39,119 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:39,120 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:39,177 INFO] Loading dataset from data/data.train.0.pt\n",
            "[2024-10-28 09:37:39,177 INFO] number of examples: 4\n",
            "[2024-10-28 09:37:39,241 INFO] Saving checkpoint /content/sample_data_step_70.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py -model \"/content/sample_data_step_70.pt\" -src \"/content/drive/MyDrive/QLDA/src_test.txt\" -output \"/content/sample_data/pred_mlp.txt\" -verbose \\\n",
        "                    --batch_size 1 --gpu 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4oDWgZdd9td",
        "outputId": "2b1ef80d-b663-4aef-e686-d4656812219b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/OpenNMT-py/onmt/modules/sparse_activations.py:48: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, dim=0):\n",
            "/content/OpenNMT-py/onmt/modules/sparse_activations.py:68: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/content/OpenNMT-py/onmt/modules/sparse_losses.py:13: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, target):\n",
            "/content/OpenNMT-py/onmt/modules/sparse_losses.py:37: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/content/OpenNMT-py/onmt/models/sru.py:397: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(self, u, x, bias, init=None, mask_h=None):\n",
            "/content/OpenNMT-py/onmt/models/sru.py:443: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(self, grad_h, grad_last):\n",
            "/content/OpenNMT-py/onmt/model_builder.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
            "  warnings.warn(\n",
            "[2024-10-28 09:37:43,224 INFO] Translating shard 0.\n",
            "/content/OpenNMT-py/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
            "[2024-10-28 09:37:44,171 INFO] \n",
            "SENT 1: ['a', 'california', 'woman', 'is', 'suing', 'sephora', 'after', 'claiming', 'she', 'contracted', 'herpes', 'from', 'using', 'one', 'of', 'the', 'cosmetic', 'store', 'lipstick', 'samples', 'the', 'woman', 'said', 'she', 'visited', 'a', 'store', 'in', 'hollywood', 'back', 'in', 'october', '2015', 'and', 'used', 'one', 'of', 'the', 'sample', 'lipsticks', 'on', 'display', 'she', 'claims', 'she', 'ended', 'up', 'with', 'the', 'incurable', 'disease', 'on', 'her', 'lip', 'according', 'to', 'court', 'documents', 'obtained', 'by', 'tmz', 'the', 'woman', 'who', 'hasnt', 'been', 'identified', 'says', 'doctors', 'diagnosed', 'her', 'with', 'the', 'std', 'she', 'said', 'she', 'did', 'not', 'have', 'herpes', 'before', 'going', 'to', 'the', 'sephora', 'store', 'the', 'lawsuit', 'claims', 'sephora', 'failed', 'to', 'warn', 'her', 'as', 'well', 'as', 'other', 'customers', 'of', 'the', 'dangers', 'of', 'using', 'lipstick', 'samples', 'she', 'said', 'other']\n",
            "PRED 1: utah man to is endangered species respect five months my my slain to to to\n",
            "PRED SCORE: -33.5537\n",
            "\n",
            "[2024-10-28 09:37:44,198 INFO] \n",
            "SENT 2: ['a', 'family', 'christmas', 'has', 'been', 'ruined', 'after', 'an', 'airline', 'refused', 'to', 'let', 'an', 'eight', 'year', 'old', 'boy', 'fly', 'to', 'see', 'his', 'sick', 'grandparents', 'in', 'south', 'africa', 'because', 'he', 'had', 'a', 'different', 'surname', 'to', 'his', 'mother', 'nurse', 'rachelle', 'stassen', 'and', 'husband', 'martin', 'booked', 'a', 'dream', '4000', 'holiday', 'to', 'his', 'native', 'home', 'two', 'years', 'ago', 'and', 'were', 'due', 'to', 'fly', 'out', 'on', 'saturday', 'but', 'when', 'they', 'arrived', 'at', 'the', 'check', 'in', 'desk', 'with', 'her', 'son', 'riley', 'fernandez', 'from', 'a', 'previous', 'marriage', 'turkish', 'airlines', 'staff', 'refused', 'to', 'let', 'them', 'on', 'the', 'flight', 'and', 'even', 'when', 'riley', 'father', 'jason', 'turned', 'up', 'at', 'manchester', 'airport', 'to', 'give', 'permission', 'in', 'person', 'staff', 'refused', 'to', 'back', 'down']\n",
            "PRED 2: utah man to is endangered species respect five months my my slain to to to\n",
            "PRED SCORE: -33.5329\n",
            "\n",
            "[2024-10-28 09:37:44,230 INFO] \n",
            "SENT 3: ['a', 'college', 'basketball', 'coach', 'hopeful', 'claims', 'she', 'was', 'denied', 'a', 'job', 'because', 'she', 'is', 'no', 'longer', 'gay', 'after', 'her', 'new', 'boss', 'saw', 'a', 'video', 'of', 'her', 'blasting', 'homosexuality', 'as', 'wrong', 'camille', 'lenoir', 'was', 'thrilled', 'when', 'her', 'former', 'college', 'coach', 'mark', 'trakh', 'had', 'offered', 'her', 'an', 'assistant', 'position', 'at', 'new', 'mexico', 'state', 'university', 'on', 'april', '24', '2016', 'but', 'just', 'two', 'days', 'before', 'she', 'was', 'due', 'to', 'board', 'a', 'plane', 'to', 'start', 'her', 'new', 'role', 'in', 'new', 'mexico', 'she', 'says', 'she', 'got', 'call', 'from', 'trakh', '62', 'rescinding', 'her', 'offer', 'the', 'washington', 'post', 'reports', 'hed', 'seen', 'a', 'video', 'she', 'posted', 'online', 'where', 'she', 'described', 'homosexuality', 'was', 'wrong', 'and', 'not', 'worth', 'losing', 'your', 'soul']\n",
            "PRED 3: utah man to is endangered species respect five months my my slain to to to\n",
            "PRED SCORE: -33.5576\n",
            "\n",
            "[2024-10-28 09:37:44,258 INFO] \n",
            "SENT 4: ['keep', 'the', 'cold', 'out', 'with', 'these', 'authentic', 'and', 'easy', 'curries', 'serves', '4', 'to', 'serve', 'for', 'a', 'stronger', 'tasting', 'dish', 'try', 'replacing', 'the', 'prawns', 'with', '8', 'mackerel', 'fillets', 'that', 'have', 'been', 'halved', 'or', 'cut', 'into', 'large', 'pieces', 'if', 'the', 'prawns', 'havent', 'been', 'deveined', 'do', 'this', 'by', 'making', 'a', 'shallow', 'cut', 'along', 'the', 'length', 'of', 'the', 'black', 'line', 'lrb', 'intestinal', 'tract', 'rrb', 'running', 'down', 'the', 'back', 'of', 'each', 'prawn', 'then', 'lift', 'it', 'out', 'using', 'the', 'tip', 'of', 'the', 'knife', 'place', 'the', 'prawns', 'in', 'a', 'bowl', 'and', 'sprinkle', 'over', 'the', 'turmeric', 'vinegar', 'and', 'salt', 'mix', 'and', 'set', 'aside', 'for', '15', '20', 'minutes', 'while', 'you', 'get', 'the', 'curry', 'ready', 'heat', 'the', 'oil', 'in', 'a']\n",
            "PRED 4: utah man to is endangered species respect five months my my slain to to to\n",
            "PRED SCORE: -33.5779\n",
            "\n",
            "[2024-10-28 09:37:44,258 INFO] PRED AVG SCORE: -2.2370, PRED PPL: 9.3655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BLEU-SCORE"
      ],
      "metadata": {
        "id": "Chq261WreOYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl\n",
        "!chmod +x multi-bleu.perl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtF30UxWeRGw",
        "outputId": "a62ffc38-1833-4eeb-a8a8-25a93be07563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-28 09:37:44--  https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5234 (5.1K) [text/plain]\n",
            "Saving to: ‘multi-bleu.perl’\n",
            "\n",
            "\rmulti-bleu.perl       0%[                    ]       0  --.-KB/s               \rmulti-bleu.perl     100%[===================>]   5.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-28 09:37:45 (44.2 MB/s) - ‘multi-bleu.perl’ saved [5234/5234]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!perl multi-bleu.perl \"/content/drive/MyDrive/QLDA/tgt-test.txt\" < \"/content/sample_data/pred_mlp.txt\" >> \"/content/sample_data/bleuoutput_mlp.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VAoBuoWeUbv",
        "outputId": "8fecfa78-1239-4442-f743-a5b8b72e96db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use of uninitialized value in division (/) at multi-bleu.perl line 139, <STDIN> line 4.\n",
            "Use of uninitialized value in division (/) at multi-bleu.perl line 139, <STDIN> line 4.\n",
            "Use of uninitialized value in division (/) at multi-bleu.perl line 139, <STDIN> line 4.\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/sample_data/bleuoutput_mlp.txt\",'r') as file:\n",
        "  print(file.readlines())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta2uFpYHeWGK",
        "outputId": "5e742edd-cdef-4777-dc68-fd59ed006c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BLEU = 0.00, 6.7/0.0/0.0/0.0 (BP=0.597, ratio=0.659, hyp_len=60, ref_len=91)\\n']\n"
          ]
        }
      ]
    }
  ]
}